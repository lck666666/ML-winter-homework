{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_dim': 10,  'output_dim': 1, 'hidden_units': [200, 200, 200],  'learning_rate': 1e-05, 'learning_rateDecay': 2e-07, 'train_size': 1000, 'batch_size': 1000, 'test_size': 1000, 'x_start': 0, 'x_end': 1, 'epotch': 10000}\n",
      "Net(\n",
      "  (layer): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=200, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=200, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuchangkundeimac/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train  step: 100 , loss: 36.35396194458008\n",
      "test   step: 100 , loss: 36.832733154296875\n",
      "------------------------------------------------\n",
      "train  step: 200 , loss: 30.399269104003906\n",
      "test   step: 200 , loss: 30.85252571105957\n",
      "------------------------------------------------\n",
      "train  step: 300 , loss: 23.527374267578125\n",
      "test   step: 300 , loss: 23.947086334228516\n",
      "------------------------------------------------\n",
      "train  step: 400 , loss: 16.874576568603516\n",
      "test   step: 400 , loss: 17.25501823425293\n",
      "------------------------------------------------\n",
      "train  step: 500 , loss: 11.581501007080078\n",
      "test   step: 500 , loss: 11.921741485595703\n",
      "------------------------------------------------\n",
      "train  step: 600 , loss: 8.04482650756836\n",
      "test   step: 600 , loss: 8.349196434020996\n",
      "------------------------------------------------\n",
      "train  step: 700 , loss: 5.976309776306152\n",
      "test   step: 700 , loss: 6.251895904541016\n",
      "------------------------------------------------\n",
      "train  step: 800 , loss: 4.875844478607178\n",
      "test   step: 800 , loss: 5.129859447479248\n",
      "------------------------------------------------\n",
      "train  step: 900 , loss: 4.330624103546143\n",
      "test   step: 900 , loss: 4.569106101989746\n",
      "------------------------------------------------\n",
      "train  step: 1000 , loss: 4.076935291290283\n",
      "test   step: 1000 , loss: 4.304506778717041\n",
      "------------------------------------------------\n",
      "train  step: 1100 , loss: 3.9662797451019287\n",
      "test   step: 1100 , loss: 4.186327934265137\n",
      "------------------------------------------------\n",
      "train  step: 1200 , loss: 3.921355724334717\n",
      "test   step: 1200 , loss: 4.136298179626465\n",
      "------------------------------------------------\n",
      "train  step: 1300 , loss: 3.9045090675354004\n",
      "test   step: 1300 , loss: 4.116023540496826\n",
      "------------------------------------------------\n",
      "train  step: 1400 , loss: 3.8986315727233887\n",
      "test   step: 1400 , loss: 4.107852458953857\n",
      "------------------------------------------------\n",
      "train  step: 1500 , loss: 3.896557569503784\n",
      "test   step: 1500 , loss: 4.104218482971191\n",
      "------------------------------------------------\n",
      "train  step: 1600 , loss: 3.895565986633301\n",
      "test   step: 1600 , loss: 4.1021199226379395\n",
      "------------------------------------------------\n",
      "train  step: 1700 , loss: 3.894749641418457\n",
      "test   step: 1700 , loss: 4.100461006164551\n",
      "------------------------------------------------\n",
      "train  step: 1800 , loss: 3.8938708305358887\n",
      "test   step: 1800 , loss: 4.098874568939209\n",
      "------------------------------------------------\n",
      "train  step: 1900 , loss: 3.892892837524414\n",
      "test   step: 1900 , loss: 4.097248554229736\n",
      "------------------------------------------------\n",
      "train  step: 2000 , loss: 3.8918323516845703\n",
      "test   step: 2000 , loss: 4.095553398132324\n",
      "------------------------------------------------\n",
      "train  step: 2100 , loss: 3.8907089233398438\n",
      "test   step: 2100 , loss: 4.09377908706665\n",
      "------------------------------------------------\n",
      "train  step: 2200 , loss: 3.8895299434661865\n",
      "test   step: 2200 , loss: 4.091921329498291\n",
      "------------------------------------------------\n",
      "train  step: 2300 , loss: 3.8883092403411865\n",
      "test   step: 2300 , loss: 4.089981555938721\n",
      "------------------------------------------------\n",
      "train  step: 2400 , loss: 3.8870463371276855\n",
      "test   step: 2400 , loss: 4.087958335876465\n",
      "------------------------------------------------\n",
      "train  step: 2500 , loss: 3.88574481010437\n",
      "test   step: 2500 , loss: 4.085849761962891\n",
      "------------------------------------------------\n",
      "train  step: 2600 , loss: 3.884409189224243\n",
      "test   step: 2600 , loss: 4.08365535736084\n",
      "------------------------------------------------\n",
      "train  step: 2700 , loss: 3.883039712905884\n",
      "test   step: 2700 , loss: 4.081375598907471\n",
      "------------------------------------------------\n",
      "train  step: 2800 , loss: 3.8816416263580322\n",
      "test   step: 2800 , loss: 4.079010009765625\n",
      "------------------------------------------------\n",
      "train  step: 2900 , loss: 3.880213975906372\n",
      "test   step: 2900 , loss: 4.076560974121094\n",
      "------------------------------------------------\n",
      "train  step: 3000 , loss: 3.8787641525268555\n",
      "test   step: 3000 , loss: 4.074026107788086\n",
      "------------------------------------------------\n",
      "train  step: 3100 , loss: 3.877293586730957\n",
      "test   step: 3100 , loss: 4.071409225463867\n",
      "------------------------------------------------\n",
      "train  step: 3200 , loss: 3.875808000564575\n",
      "test   step: 3200 , loss: 4.068711280822754\n",
      "------------------------------------------------\n",
      "train  step: 3300 , loss: 3.874312400817871\n",
      "test   step: 3300 , loss: 4.065934658050537\n",
      "------------------------------------------------\n",
      "train  step: 3400 , loss: 3.8728134632110596\n",
      "test   step: 3400 , loss: 4.063082695007324\n",
      "------------------------------------------------\n",
      "train  step: 3500 , loss: 3.871314287185669\n",
      "test   step: 3500 , loss: 4.060159683227539\n",
      "------------------------------------------------\n",
      "train  step: 3600 , loss: 3.8698267936706543\n",
      "test   step: 3600 , loss: 4.05717134475708\n",
      "------------------------------------------------\n",
      "train  step: 3700 , loss: 3.8683550357818604\n",
      "test   step: 3700 , loss: 4.054124355316162\n",
      "------------------------------------------------\n",
      "train  step: 3800 , loss: 3.866910457611084\n",
      "test   step: 3800 , loss: 4.051024436950684\n",
      "------------------------------------------------\n",
      "train  step: 3900 , loss: 3.865502119064331\n",
      "test   step: 3900 , loss: 4.047879695892334\n",
      "------------------------------------------------\n",
      "train  step: 4000 , loss: 3.8641371726989746\n",
      "test   step: 4000 , loss: 4.044701099395752\n",
      "------------------------------------------------\n",
      "train  step: 4100 , loss: 3.862830400466919\n",
      "test   step: 4100 , loss: 4.041500091552734\n",
      "------------------------------------------------\n",
      "train  step: 4200 , loss: 3.861588716506958\n",
      "test   step: 4200 , loss: 4.0382866859436035\n",
      "------------------------------------------------\n",
      "train  step: 4300 , loss: 3.860424280166626\n",
      "test   step: 4300 , loss: 4.035076141357422\n",
      "------------------------------------------------\n",
      "train  step: 4400 , loss: 3.859351873397827\n",
      "test   step: 4400 , loss: 4.031881809234619\n",
      "------------------------------------------------\n",
      "train  step: 4500 , loss: 3.8583767414093018\n",
      "test   step: 4500 , loss: 4.028717041015625\n",
      "------------------------------------------------\n",
      "train  step: 4600 , loss: 3.8575124740600586\n",
      "test   step: 4600 , loss: 4.025601387023926\n",
      "------------------------------------------------\n",
      "train  step: 4700 , loss: 3.8567657470703125\n",
      "test   step: 4700 , loss: 4.022547721862793\n",
      "------------------------------------------------\n",
      "train  step: 4800 , loss: 3.8561453819274902\n",
      "test   step: 4800 , loss: 4.019571304321289\n",
      "------------------------------------------------\n",
      "train  step: 4900 , loss: 3.8556573390960693\n",
      "test   step: 4900 , loss: 4.016690254211426\n",
      "------------------------------------------------\n",
      "train  step: 5000 , loss: 3.8553032875061035\n",
      "test   step: 5000 , loss: 4.013918399810791\n",
      "------------------------------------------------\n",
      "train  step: 5100 , loss: 3.8550848960876465\n",
      "test   step: 5100 , loss: 4.011268138885498\n",
      "------------------------------------------------\n",
      "train  step: 5200 , loss: 3.854999542236328\n",
      "test   step: 5200 , loss: 4.008749485015869\n",
      "------------------------------------------------\n",
      "train  step: 5300 , loss: 3.855043411254883\n",
      "test   step: 5300 , loss: 4.006374835968018\n",
      "------------------------------------------------\n",
      "train  step: 5400 , loss: 3.85520601272583\n",
      "test   step: 5400 , loss: 4.004148960113525\n",
      "------------------------------------------------\n",
      "train  step: 5500 , loss: 3.8554775714874268\n",
      "test   step: 5500 , loss: 4.0020751953125\n",
      "------------------------------------------------\n",
      "train  step: 5600 , loss: 3.8558452129364014\n",
      "test   step: 5600 , loss: 4.000155925750732\n",
      "------------------------------------------------\n",
      "train  step: 5700 , loss: 3.8562934398651123\n",
      "test   step: 5700 , loss: 3.9983882904052734\n",
      "------------------------------------------------\n",
      "train  step: 5800 , loss: 3.8568031787872314\n",
      "test   step: 5800 , loss: 3.996767044067383\n",
      "------------------------------------------------\n",
      "train  step: 5900 , loss: 3.857355833053589\n",
      "test   step: 5900 , loss: 3.9952845573425293\n",
      "------------------------------------------------\n",
      "train  step: 6000 , loss: 3.8579306602478027\n",
      "test   step: 6000 , loss: 3.993932008743286\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train  step: 6100 , loss: 3.858506441116333\n",
      "test   step: 6100 , loss: 3.9926931858062744\n",
      "------------------------------------------------\n",
      "train  step: 6200 , loss: 3.8590664863586426\n",
      "test   step: 6200 , loss: 3.9915595054626465\n",
      "------------------------------------------------\n",
      "train  step: 6300 , loss: 3.8595871925354004\n",
      "test   step: 6300 , loss: 3.990511894226074\n",
      "------------------------------------------------\n",
      "train  step: 6400 , loss: 3.8600528240203857\n",
      "test   step: 6400 , loss: 3.989535331726074\n",
      "------------------------------------------------\n",
      "train  step: 6500 , loss: 3.8604509830474854\n",
      "test   step: 6500 , loss: 3.988615036010742\n",
      "------------------------------------------------\n",
      "train  step: 6600 , loss: 3.860765218734741\n",
      "test   step: 6600 , loss: 3.9877374172210693\n",
      "------------------------------------------------\n",
      "train  step: 6700 , loss: 3.860989570617676\n",
      "test   step: 6700 , loss: 3.986886501312256\n",
      "------------------------------------------------\n",
      "train  step: 6800 , loss: 3.8611178398132324\n",
      "test   step: 6800 , loss: 3.986051559448242\n",
      "------------------------------------------------\n",
      "train  step: 6900 , loss: 3.8611502647399902\n",
      "test   step: 6900 , loss: 3.9852211475372314\n",
      "------------------------------------------------\n",
      "train  step: 7000 , loss: 3.86108660697937\n",
      "test   step: 7000 , loss: 3.9843878746032715\n",
      "------------------------------------------------\n",
      "train  step: 7100 , loss: 3.8609299659729004\n",
      "test   step: 7100 , loss: 3.983544111251831\n",
      "------------------------------------------------\n",
      "train  step: 7200 , loss: 3.8606863021850586\n",
      "test   step: 7200 , loss: 3.9826855659484863\n",
      "------------------------------------------------\n",
      "train  step: 7300 , loss: 3.8603639602661133\n",
      "test   step: 7300 , loss: 3.981806993484497\n",
      "------------------------------------------------\n",
      "train  step: 7400 , loss: 3.8599698543548584\n",
      "test   step: 7400 , loss: 3.9809045791625977\n",
      "------------------------------------------------\n",
      "train  step: 7500 , loss: 3.8595099449157715\n",
      "test   step: 7500 , loss: 3.979975700378418\n",
      "------------------------------------------------\n",
      "train  step: 7600 , loss: 3.858991861343384\n",
      "test   step: 7600 , loss: 3.9790172576904297\n",
      "------------------------------------------------\n",
      "train  step: 7700 , loss: 3.8584237098693848\n",
      "test   step: 7700 , loss: 3.978026866912842\n",
      "------------------------------------------------\n",
      "train  step: 7800 , loss: 3.857809066772461\n",
      "test   step: 7800 , loss: 3.977001905441284\n",
      "------------------------------------------------\n",
      "train  step: 7900 , loss: 3.8571529388427734\n",
      "test   step: 7900 , loss: 3.975940704345703\n",
      "------------------------------------------------\n",
      "train  step: 8000 , loss: 3.8564577102661133\n",
      "test   step: 8000 , loss: 3.9748380184173584\n",
      "------------------------------------------------\n",
      "train  step: 8100 , loss: 3.855724573135376\n",
      "test   step: 8100 , loss: 3.9736905097961426\n",
      "------------------------------------------------\n",
      "train  step: 8200 , loss: 3.8549530506134033\n",
      "test   step: 8200 , loss: 3.9724955558776855\n",
      "------------------------------------------------\n",
      "train  step: 8300 , loss: 3.8541438579559326\n",
      "test   step: 8300 , loss: 3.9712448120117188\n",
      "------------------------------------------------\n",
      "train  step: 8400 , loss: 3.85329270362854\n",
      "test   step: 8400 , loss: 3.9699342250823975\n",
      "------------------------------------------------\n",
      "train  step: 8500 , loss: 3.852400064468384\n",
      "test   step: 8500 , loss: 3.9685568809509277\n",
      "------------------------------------------------\n",
      "train  step: 8600 , loss: 3.8514585494995117\n",
      "test   step: 8600 , loss: 3.9671032428741455\n",
      "------------------------------------------------\n",
      "train  step: 8700 , loss: 3.8504638671875\n",
      "test   step: 8700 , loss: 3.9655637741088867\n",
      "------------------------------------------------\n",
      "train  step: 8800 , loss: 3.8494091033935547\n",
      "test   step: 8800 , loss: 3.9639267921447754\n",
      "------------------------------------------------\n",
      "train  step: 8900 , loss: 3.8482871055603027\n",
      "test   step: 8900 , loss: 3.9621782302856445\n",
      "------------------------------------------------\n",
      "train  step: 9000 , loss: 3.847088575363159\n",
      "test   step: 9000 , loss: 3.9603004455566406\n",
      "------------------------------------------------\n",
      "train  step: 9100 , loss: 3.8458034992218018\n",
      "test   step: 9100 , loss: 3.958275556564331\n",
      "------------------------------------------------\n",
      "train  step: 9200 , loss: 3.8444201946258545\n",
      "test   step: 9200 , loss: 3.9560813903808594\n",
      "------------------------------------------------\n",
      "train  step: 9300 , loss: 3.842923164367676\n",
      "test   step: 9300 , loss: 3.953690528869629\n",
      "------------------------------------------------\n",
      "train  step: 9400 , loss: 3.8413002490997314\n",
      "test   step: 9400 , loss: 3.951076030731201\n",
      "------------------------------------------------\n",
      "train  step: 9500 , loss: 3.839529037475586\n",
      "test   step: 9500 , loss: 3.9482004642486572\n",
      "------------------------------------------------\n",
      "train  step: 9600 , loss: 3.837590217590332\n",
      "test   step: 9600 , loss: 3.945026159286499\n",
      "------------------------------------------------\n",
      "train  step: 9700 , loss: 3.8354599475860596\n",
      "test   step: 9700 , loss: 3.9415078163146973\n",
      "------------------------------------------------\n",
      "train  step: 9800 , loss: 3.83311128616333\n",
      "test   step: 9800 , loss: 3.9375953674316406\n",
      "------------------------------------------------\n",
      "train  step: 9900 , loss: 3.830514907836914\n",
      "test   step: 9900 , loss: 3.9332332611083984\n",
      "------------------------------------------------\n",
      "train  step: 10000 , loss: 3.8276355266571045\n",
      "test   step: 10000 , loss: 3.9283528327941895\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "%matplotlib inline\n",
    "R_variable={}  ### used for saved all parameters and data\n",
    "R_variable['input_dim']=10\n",
    "R_variable['output_dim']=1\n",
    "R_variable['hidden_units']=[200,200,200]\n",
    "### initialization standard deviation\n",
    "R_variable['learning_rate']=1e-5\n",
    "R_variable['learning_rateDecay']=2e-7\n",
    "R_variable['train_size']=1000;  ### training size\n",
    "R_variable['batch_size']=R_variable['train_size'] # int(np.floor(R_variable['train_size'])) ### batch size\n",
    "R_variable['test_size']=R_variable['train_size']  ### test size\n",
    "R_variable['x_start']=0 #math.pi*3 ### start point of input\n",
    "R_variable['x_end']=1  #6.28/4 #math.pi*3  ### end point of input\n",
    "R_variable['epotch'] = 10000\n",
    "print(R_variable) \n",
    "\n",
    "# x = torch.randn(R_variable['train_size']=1000, R_variable['input_dim'])\n",
    "# y = torch.randn(R_variable['train_size']=1000, R_variable['output_dim']) \n",
    "#------------------------------PDE para-----------------------------\n",
    "n = 1000\n",
    "R_variable['test_size'] = n\n",
    "nw = 100\n",
    "R_variable['boundary_size'] = nw\n",
    "beta = 1000\n",
    "if R_variable['input_dim']==1:\n",
    "    x_train =np.reshape(np.linspace(R_variable['x_start'], R_variable['x_end'], num=R_variable['test_size'],\n",
    "                                                      endpoint=True),[R_variable['test_size'],1])\n",
    "    x_test = np.reshape(np.linspace(R_variable['x_start'], R_variable['x_end'], num=R_variable['test_size'],\n",
    "                                                      endpoint=True),[R_variable['test_size'],1])\n",
    "    x_train = x_train.astype(np.float32)\n",
    "    x_test = x_test.astype(np.float32)\n",
    "#-------------------Boundry x for PDE function-----------------------\n",
    "    xBou = np.reshape(np.linspace(R_variable['x_start'], R_variable['x_end'], num=R_variable['test_size'],\n",
    "                                                      endpoint=True),[R_variable['test_size'],1])\n",
    "    xBou = xBou.astype(np.float32)\n",
    "else:\n",
    "    x_train =np.random.rand(R_variable['test_size'],R_variable['input_dim'])*(R_variable['x_end']-R_variable['x_start'])+R_variable['x_start']\n",
    "    x_train = x_train.astype(np.float32)\n",
    "    x_test =np.random.rand(R_variable['test_size'],R_variable['input_dim'])*(R_variable['x_end']-R_variable['x_start'])+R_variable['x_start']\n",
    "    x_test = x_test.astype(np.float32)\n",
    "#-------------------Boundry x for PDE function-----------------------\n",
    "    xBou = np.random.rand(R_variable['boundary_size'],R_variable['input_dim'])*(R_variable['x_end']-R_variable['x_start'])+R_variable['x_start']\n",
    "    a = np.random.randint(0,R_variable['input_dim'] , 1)\n",
    "    xBou[:,a] = np.random.randint(0,1)\n",
    "    xBou = xBou.astype(np.float32)\n",
    "\n",
    "x_train = torch.tensor(x_train,requires_grad=True)\n",
    "x_test = torch.tensor(x_test)\n",
    "xBou = torch.tensor(x_test)\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_dim,hidden_units,out_dim):\n",
    "        super(Net, self).__init__()  \n",
    "        self.layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_dim,hidden_units[0]), \n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_units[0],hidden_units[1]),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_units[1],hidden_units[2]),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_units[2], out_dim)\n",
    "             )\n",
    "    def forward(self, x):\n",
    "        y_pre = self.layer(x)\n",
    "        return y_pre\n",
    "\n",
    "    \n",
    "def get_y_func(xs):\n",
    "    tmp=0\n",
    "    x = xs.detach().numpy()\n",
    "    for ii in range(R_variable['input_dim']):\n",
    "        tmp += np.sin(x[:,ii:ii+1]) + np.sin(10*x[:,ii:ii+1])\n",
    "    return torch.tensor(tmp)\n",
    "    \n",
    "def  PDE_g_func(xIn):\n",
    "    tmp = 0\n",
    "    xIn = xIn.detach().numpy()\n",
    "    for ii in range(R_variable['input_dim']):\n",
    "        tmp += np.sin(xIn[:,ii:ii+1]) + 100*np.sin(10*xIn[:,ii:ii+1])\n",
    "    return torch.tensor(tmp)\n",
    "\n",
    "def  PDE_gw_func(xBou):\n",
    "    tmp = 0\n",
    "    xBou = xBou.detach().numpy()\n",
    "    for ii in range(R_variable['input_dim']):\n",
    "        tmp += np.sin(xBou[:,ii:ii+1]) + np.sin(10*xBou[:,ii:ii+1])\n",
    "    return torch.tensor(tmp)\n",
    "\n",
    "y_true_test = get_y_func(x_test)\n",
    "\n",
    "\n",
    "def ploty(ytr,yte,ep):\n",
    "        \n",
    "        if R_variable['input_dim']==2:\n",
    "            # Make data.\n",
    "            X = np.arange(R_variable['x_start'], R_variable['x_end'], 0.1)\n",
    "            Y = np.arange(R_variable['x_start'], R_variable['x_end'], 0.1)\n",
    "            X, Y = np.meshgrid(X, Y)\n",
    "            xy=np.concatenate((np.reshape(X,[-1,1]),np.reshape(Y,[-1,1])),axis=1)\n",
    "            Z = np.reshape(get_y_func(xy),[len(X),-1])\n",
    "            fp = plt.figure()\n",
    "            ax = fp.gca(projection='3d')\n",
    "            # Plot the surface.\n",
    "            surf = ax.plot_surface(X, Y, Z-np.min(Z), cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
    "            # Customize the z axis.\n",
    "            #ax.set_zlim(-2.01, 2.01)\n",
    "            ax.zaxis.set_major_locator(LinearLocator(5))\n",
    "            ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "            # Add a color bar which maps values to colors.\n",
    "            fp.colorbar(surf, shrink=0.5, aspect=5)\n",
    "            y = ytr.detach().numpy()\n",
    "            ax.scatter(x_train[:,0], x_train[:,1], y-np.min(y))\n",
    "#             plt.show()\n",
    "            plt.savefig('/Users/liuchangkundeimac/Desktop/pictures/picture%d.png'%ep)\n",
    "    \n",
    "        if R_variable['input_dim']==1:\n",
    "            plt.figure()\n",
    "            ax = plt.gca()\n",
    "            y1 = yte.detach().numpy()\n",
    "            y2 = ytr.detach().numpy()\n",
    "            y3 = y_true_test.numpy()\n",
    "            test_inputs = np.array(x_test)\n",
    "            train_inputs = np.array(x_train)\n",
    "            plt.plot(test_inputs,y1,'ro',label='Test')\n",
    "            plt.plot(train_inputs,y2,'g*',label='Train')\n",
    "            plt.plot(test_inputs,y3,'b*',label='True')\n",
    "            plt.title('g2u',fontsize=15)        \n",
    "            plt.legend(fontsize=18) \n",
    "#             plt.show()\n",
    "            plt.savefig('/Users/liuchangkundeimac/Desktop/pictures/picture%d.png'%ep)\n",
    "    \n",
    "def plotloss(loss_test,loss_train):\n",
    "    y1 = loss_test\n",
    "    y2 = loss_train\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "    plt.plot(y1,'ro',label='Test')\n",
    "    plt.plot(y2,'g*',label='Train')\n",
    "#     ax.set_xscale('log')\n",
    "    ax.set_yscale('log')                \n",
    "    plt.legend(fontsize=18)\n",
    "    plt.title('lossy',fontsize=15)\n",
    "#     plt.show()\n",
    "    plt.savefig('/Users/liuchangkundeimac/Desktop/pictures/loss.png')\n",
    "    \n",
    "model = Net(R_variable['input_dim'],R_variable['hidden_units'], R_variable['output_dim'])     # define the network\n",
    "print(model)  # net architecture\n",
    "\n",
    "def train_test(model):\n",
    "    loss_train = []\n",
    "    loss_test = []\n",
    "    # Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "    # in the SGD constructor will contain the learnable parameters of the two\n",
    "    # nn.Linear modules which are members of the model.\n",
    "    criterion = torch.nn.MSELoss(reduction='sum')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=R_variable['learning_rate'],betas=(0.9, 0.999),eps=1e-08, weight_decay=0)\n",
    "    for ep in range(R_variable['epotch']):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred_train = model(x_train)\n",
    "        y_pred_trainB = model(xBou)\n",
    "        y_pred_test = model.forward(x_test)\n",
    "# #         # Compute and print loss\n",
    "        y_train = get_y_func(x_train)\n",
    "        y_test = get_y_func(x_test)\n",
    "        z = torch.sum(y_pred_train)\n",
    "        grad_x = torch.autograd.grad(z,x_train,create_graph=True,retain_graph=True)\n",
    "        grad_grad_x = torch.autograd.grad(torch.sum(grad_x[0]),x_train,retain_graph=True)\n",
    "        \n",
    "        delth = grad_grad_x[0]\n",
    "        for ii in range(R_variable['input_dim']):\n",
    "            tmp = 0\n",
    "            tmp += delth[:,ii:ii+1]\n",
    "        LSE1 = 1/n * torch.sum((tmp + PDE_g_func(x_train))**2)\n",
    "        LSE2 =  beta*1/nw*torch.sum((y_pred_trainB-PDE_gw_func(xBou))**2)\n",
    "        loss1 = LSE1 + LSE2\n",
    "        lossy1 = 1/(n+nw)*torch.sum((y_pred_train - y_train)**2)\n",
    "        lossy2 = 1/(n+nw)*torch.sum((y_pred_test - y_test)**2)\n",
    "        loss2 = torch.sum(1/n*(tmp + PDE_g_func(x_test))**2) + torch.sum(LSE2)\n",
    "        \n",
    "        loss_train.append(lossy1)\n",
    "        loss_test.append(lossy2)\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss1.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        if (ep+1)%100==0:\n",
    "            print(\"train  step: {0} , loss: {1}\".format(ep+1,lossy1.item()))       \n",
    "            print(\"test   step: {0} , loss: {1}\".format(ep+1,lossy2.item()))\n",
    "            print(\"------------------------------------------------\")\n",
    "#             ploty(y_pred_train,y_pred_test,ep)\n",
    "            \n",
    "    plotloss(loss_test,loss_train)\n",
    "    \n",
    "train_test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
