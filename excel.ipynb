{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FolderName': 'fitnd/22519/', 'input_dim': 50, 'epsion': 0.1, 'output_dim': 1, 'ActFuc': 5, 'hidden_units': [200, 200, 200], 'astddev': 0.22360679774997896, 'bstddev': 0.22360679774997896, 'ASI': 0, 'learning_rate': 5e-06, 'learning_rateDecay': 2e-07, 'seed': 0, 'train_size': 5000, 'batch_size': 5000, 'test_size': 5000, 'x_start': -1.5707963267948966, 'x_end': 1.5707963267948966, 'tol': -10000000000.0, 'Total_Step': 10000}\n",
      "add_hidden: [50, 200, 200, 200]\n",
      "input_dim:50, output_dim:200\n",
      "sigmoid\n",
      "input_dim:200, output_dim:200\n",
      "sigmoid\n",
      "input_dim:200, output_dim:200\n",
      "sigmoid\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from fitnd/22519/model/model.ckpt\n",
      "initial 3.6337886\n",
      "time elapse: 0.646\n",
      "model, epoch: 0, test loss: 31.871696\n",
      "model, epoch: 0, train loss: 32.502132\n",
      "time elapse: 31.779\n",
      "model, epoch: 500, test loss: 25.094278\n",
      "model, epoch: 500, train loss: 25.342659\n",
      "time elapse: 64.654\n",
      "model, epoch: 1000, test loss: 24.464697\n",
      "model, epoch: 1000, train loss: 24.544922\n",
      "time elapse: 97.418\n",
      "model, epoch: 1500, test loss: 24.464159\n",
      "model, epoch: 1500, train loss: 24.487883\n",
      "time elapse: 130.461\n",
      "model, epoch: 2000, test loss: 24.460835\n",
      "model, epoch: 2000, train loss: 24.447809\n",
      "time elapse: 162.028\n",
      "model, epoch: 2500, test loss: 24.457624\n",
      "model, epoch: 2500, train loss: 24.406010\n",
      "time elapse: 192.333\n",
      "model, epoch: 3000, test loss: 24.456341\n",
      "model, epoch: 3000, train loss: 24.361980\n",
      "time elapse: 222.796\n",
      "model, epoch: 3500, test loss: 24.457775\n",
      "model, epoch: 3500, train loss: 24.315340\n",
      "time elapse: 253.486\n",
      "model, epoch: 4000, test loss: 24.462982\n",
      "model, epoch: 4000, train loss: 24.265932\n",
      "time elapse: 286.815\n",
      "model, epoch: 4500, test loss: 24.473301\n",
      "model, epoch: 4500, train loss: 24.213856\n",
      "time elapse: 318.342\n",
      "model, epoch: 5000, test loss: 24.490095\n",
      "model, epoch: 5000, train loss: 24.159599\n",
      "time elapse: 350.804\n",
      "model, epoch: 5500, test loss: 24.514071\n",
      "model, epoch: 5500, train loss: 24.104034\n",
      "time elapse: 383.103\n",
      "model, epoch: 6000, test loss: 24.544109\n",
      "model, epoch: 6000, train loss: 24.047840\n",
      "time elapse: 416.402\n",
      "model, epoch: 6500, test loss: 24.577068\n",
      "model, epoch: 6500, train loss: 23.990747\n",
      "time elapse: 446.964\n",
      "model, epoch: 7000, test loss: 24.609474\n",
      "model, epoch: 7000, train loss: 23.931112\n",
      "time elapse: 477.511\n",
      "model, epoch: 7500, test loss: 24.639122\n",
      "model, epoch: 7500, train loss: 23.866301\n",
      "time elapse: 508.625\n",
      "model, epoch: 8000, test loss: 24.665525\n",
      "model, epoch: 8000, train loss: 23.793182\n",
      "time elapse: 542.400\n",
      "model, epoch: 8500, test loss: 24.689350\n",
      "model, epoch: 8500, train loss: 23.708744\n",
      "time elapse: 577.363\n",
      "model, epoch: 9000, test loss: 24.711245\n",
      "model, epoch: 9000, train loss: 23.610405\n",
      "time elapse: 611.391\n",
      "model, epoch: 9500, test loss: 24.731663\n",
      "model, epoch: 9500, train loss: 23.495560\n"
     ]
    }
   ],
   "source": [
    "# coding=UTF-8\n",
    "import xlrd\n",
    "import xlwt\n",
    "import os\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "matplotlib.use('Agg')   \n",
    "import pickle\n",
    "import time  \n",
    "import shutil \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt   \n",
    "import platform\n",
    "#from BasicFunc import mySaveFig, mkdir\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "#Plot Parameters\n",
    "Leftp=0.18\n",
    "Bottomp=0.18\n",
    "Widthp=0.88-Leftp\n",
    "Heightp=0.9-Bottomp\n",
    "pos=[Leftp,Bottomp,Widthp,Heightp]\n",
    "\n",
    "\n",
    "\n",
    "def mkdir(fn):\n",
    "    if not os.path.isdir(fn):\n",
    "        os.mkdir(fn)\n",
    "def mySaveFig(pltm, fntmp,fp=0,ax=0,isax=0,iseps=0,isShowPic=0):\n",
    "    if isax==1:\n",
    "        #pltm.legend(fontsize=18)\n",
    "        # plt.title(y_name,fontsize=14)\n",
    "#        ax.set_xlabel('step',fontsize=18)\n",
    "#        ax.set_ylabel('loss',fontsize=18)\n",
    "        pltm.rc('xtick',labelsize=18)\n",
    "        pltm.rc('ytick',labelsize=18)\n",
    "        ax.set_position(pos, which='both')\n",
    "    fnm='%s.png'%(fntmp)\n",
    "    pltm.savefig(fnm)\n",
    "    if iseps:\n",
    "        fnm='%s.eps'%(fntmp)\n",
    "        pltm.savefig(fnm, format='eps', dpi=600)\n",
    "    if fp!=0:\n",
    "        fp.savefig(\"%s.pdf\"%(fntmp), bbox_inches='tight')\n",
    "    if isShowPic==1:\n",
    "        pltm.show() \n",
    "    elif isShowPic==-1:\n",
    "        return\n",
    "    else:\n",
    "        pltm.close()\n",
    "        \n",
    "R_variable={}  ### used for saved all parameters and data\n",
    "### mkdir a folder to save all output\n",
    "BaseDir = 'fitnd/'\n",
    "subFolderName = '%s'%(int(np.absolute(np.random.normal([1])*100000))//int(1)) \n",
    "FolderName = '%s%s/'%(BaseDir,subFolderName)\n",
    "mkdir(BaseDir)\n",
    "mkdir(FolderName)\n",
    "mkdir('%smodel/'%(FolderName))\n",
    "R_variable['FolderName']=FolderName \n",
    "if  not platform.system()=='Windows':\n",
    "    shutil.copy(__file__,'%s%s'%(FolderName,os.path.basename(__file__)))    \n",
    "R_variable['input_dim']=50\n",
    "R_variable['epsion']=0.1\n",
    "\n",
    "#***Function we need to fit***\n",
    "def get_y_func(xs):\n",
    "    tmp=0\n",
    "    #print(\"input x_data is: \",xs)\n",
    "    for ii in range(R_variable['input_dim']):\n",
    "        #tmp+=np.cos(4*xs[:,ii:ii+1])                    #***elementwise calculate cos value***\n",
    "        #tmp+=np.cos(10*xs[:,ii:ii+1])+np.cos(50*xs[:,ii:ii+1])+np.cos(200*xs[:,ii:ii+1])\n",
    "#         tmp += (np.sin(50*xs[:,ii:ii+1])+np.cos(50*xs[:,ii:ii+1]))**2\n",
    "        tmp+=np.sin(5*xs[:,ii:ii+1]) \n",
    "    return tmp\n",
    "# 似乎是真实值，若有n个输入， f(x1,x2,...,xn)=∑(cos(4*xi))\n",
    "\n",
    "R_variable['output_dim']=1\n",
    "R_variable['ActFuc']=5   ###  0: ReLU; 1: Tanh; 3:sin;4: x**5,, 5: sigmoid  6 sigmoid derivate\n",
    "#R_variable['hidden_units']=[1500,1500,1500]\n",
    "#R_variable['hidden_units']=[500,500,500]\n",
    "R_variable['hidden_units']=[200,200,200]\n",
    "#R_variable['hidden_units']=[300,300,300,300]\n",
    "### initialization standard deviation\n",
    "R_variable['astddev']=np.sqrt(1/20) # for weight\n",
    "R_variable['bstddev']=np.sqrt(1/20)# for bias terms2\n",
    "R_variable['ASI']=0\n",
    "R_variable['learning_rate']=5e-6\n",
    "R_variable['learning_rateDecay']=2e-7\n",
    "R_variable['seed']=0\n",
    "### setup for activation function\n",
    "#***The parameters used to train the model***\n",
    "plotepoch=500\n",
    "R_variable['train_size']=5000;                    ### training size\n",
    "R_variable['batch_size']=R_variable['train_size'] # int(np.floor(R_variable['train_size'])) ### batch size\n",
    "R_variable['test_size']=R_variable['train_size']  ### test size\n",
    "R_variable['x_start']=-np.pi/2  #math.pi*3 ### start point of input\n",
    "R_variable['x_end']=np.pi/2  #6.28/4 #math.pi*3  ### end point of input\n",
    "R_variable['tol']=-1e10\n",
    "R_variable['Total_Step']=10000  ### the training step. Set a big number, if it converges, can manually stop training \n",
    "R_variable['FolderName']=FolderName   ### folder for save images\n",
    "# 上述均为训练模型的参数\n",
    "print(R_variable) \n",
    "\n",
    "\n",
    "#***Training and test data are generated here***\n",
    "# 这里生成训练、测试数据。输入为一维的话，应该是均匀地在区间里线性分布；多维的话，每一个数据均为[start, end]中的随机数\n",
    "if R_variable['input_dim']==1:\n",
    "    R_variable['test_inputs'] =np.reshape(np.linspace(R_variable['x_start'], R_variable['x_end'], \n",
    "                                                      num=R_variable['test_size'],\n",
    "                                                      endpoint=True),[R_variable['test_size'],1])\n",
    "    #n_size=R_variable['train_size']\n",
    "    R_variable['train_inputs']=np.reshape(np.linspace(R_variable['x_start'], R_variable['x_end'],\n",
    "                                                      num=R_variable['train_size'],endpoint=True),\n",
    "                                          [R_variable['train_size'],1])\n",
    "else:\n",
    "    R_variable['test_inputs']=np.random.rand(R_variable['test_size'],R_variable['input_dim'])\\\n",
    "    *(R_variable['x_end']-R_variable['x_start'])+R_variable['x_start']\n",
    "    R_variable['train_inputs']=np.random.rand(R_variable['train_size'],R_variable['input_dim'])\\\n",
    "    *(R_variable['x_end']-R_variable['x_start'])+R_variable['x_start']\n",
    "\n",
    "\n",
    "\n",
    "test_inputs=R_variable['test_inputs']\n",
    "train_inputs=R_variable['train_inputs']\n",
    "R_variable['y_true_test']= get_y_func(test_inputs)\n",
    "R_variable['y_true_train']=get_y_func(train_inputs)\n",
    "\n",
    "#plt.figure()\n",
    "#plt.plot(test_inputs,R_variable['y_true_test'])\n",
    "\n",
    "t0=time.time() \n",
    "\n",
    "def add_layer2(x,input_dim = 1,output_dim = 1,isresnet=0,astddev=0.05,\n",
    "               bstddev=0.05,ActFuc=1,seed=0, name_scope='hidden'):\n",
    "    if not seed==0:\n",
    "        tf.set_random_seed(seed)\n",
    "    \n",
    "    with tf.variable_scope(name_scope, reuse=tf.AUTO_REUSE):\n",
    "        ua_w = tf.get_variable(name='ua_w', initializer=astddev)\n",
    "        ua_b = tf.get_variable(name='ua_b', initializer=bstddev) \n",
    "        z=tf.matmul(x, ua_w) + ua_b\n",
    "        \n",
    "        \n",
    "        if ActFuc==1:\n",
    "            output_z = tf.nn.tanh(z)\n",
    "            print('tanh')\n",
    "        elif ActFuc==3:\n",
    "            output_z = tf.sin(z)\n",
    "            print('sin')\n",
    "        elif ActFuc==0:\n",
    "            output_z = tf.nn.relu(z)\n",
    "            print('relu')\n",
    "        elif ActFuc==4:\n",
    "            output_z = z**50\n",
    "            print('z**50')\n",
    "        elif ActFuc==5:\n",
    "            output_z = tf.nn.sigmoid(z)\n",
    "            print('sigmoid')\n",
    "            \n",
    "        L2Wight= tf.nn.l2_loss(ua_w) \n",
    "        if isresnet and input_dim==output_dim:\n",
    "            output_z=output_z+x\n",
    "        return output_z,ua_w,ua_b,L2Wight\n",
    "\n",
    "def getWini(hidden_units=[10,20,40],input_dim = 1,output_dim_final = 1,astddev=0.05,bstddev=0.05):\n",
    "    \n",
    "    hidden_num = len(hidden_units)\n",
    "    #print(hidden_num)\n",
    "    add_hidden = [input_dim] + hidden_units;\n",
    "    # 加入第一层的输入维数（input_dim）\n",
    "    w_Univ0=[]\n",
    "    b_Univ0=[]\n",
    "    #***Set weight and biase of each neuron***\n",
    "    for i in range(hidden_num):\n",
    "        input_dim = add_hidden[i]\n",
    "        output_dim = add_hidden[i+1]\n",
    "        ua_w=np.float32(np.random.normal(loc=0.0,scale=astddev,size=[input_dim,output_dim]))\n",
    "        ua_b=np.float32(np.random.normal(loc=0.0,scale=bstddev,size=[output_dim]))\n",
    "        w_Univ0.append(ua_w)\n",
    "        b_Univ0.append(ua_b)\n",
    "        # 构造每一层，感觉ua_w,ua_b是初始值，output = input * ua_w + ua_b\n",
    "    #***Set weight and biase of the last layer, they are random value***    \n",
    "    ua_w=np.float32(np.random.normal(loc=0.0,scale=astddev,size=[hidden_units[hidden_num-1], output_dim_final]))\n",
    "    ua_b=np.float32(np.random.normal(loc=0.0,scale=bstddev,size=[output_dim_final]))\n",
    "    w_Univ0.append(ua_w)\n",
    "    b_Univ0.append(ua_b)\n",
    "    # 同样构造最后一层，输出为output_dim_final，将矩阵初始值加入（随机值）\n",
    "    return w_Univ0, b_Univ0\n",
    "\n",
    "\n",
    "\n",
    "def univAprox2(x0, hidden_units=[10,20,40],input_dim = 1,output_dim_final = 1,\n",
    "               isresnet=0,astddev=0.05,bstddev=0.05,\n",
    "               ActFuc=0,seed=0,ASI=1):\n",
    "    if seed==0:\n",
    "        seed=time.time()\n",
    "    # The simple case is f: R -> R \n",
    "    hidden_num = len(hidden_units)\n",
    "    #print(hidden_num)\n",
    "    add_hidden = [input_dim] + hidden_units;\n",
    "    print(\"add_hidden:\",add_hidden)\n",
    "    w_Univ=[]\n",
    "    b_Univ=[] \n",
    "    L2w_all=0\n",
    "    w_Univ0, b_Univ0=getWini(hidden_units=hidden_units,input_dim = input_dim,\n",
    "                             output_dim_final= output_dim_final,astddev=astddev,bstddev=bstddev)\n",
    "    output=x0\n",
    "    for i in range(hidden_num):\n",
    "        input_dim = add_hidden[i]\n",
    "        output_dim = add_hidden[i+1]\n",
    "        print('input_dim:%s, output_dim:%s'%(input_dim,output_dim))\n",
    "        name_scope = 'hidden' + np.str(i+1)   \n",
    "        output,ua_w,ua_b,L2Wight_tmp=add_layer2(output,input_dim,output_dim,isresnet=isresnet,\n",
    "                                               astddev=w_Univ0[i],bstddev=b_Univ0[i], ActFuc=ActFuc,\n",
    "                                              seed=seed, name_scope= name_scope)\n",
    "        w_Univ.append(ua_w)\n",
    "        b_Univ.append(ua_b)\n",
    "        L2w_all = L2w_all + L2Wight_tmp\n",
    "        # 这个函数会将output进行一次非线性的activation，同时获得这一层的矩阵参数ua_w, ua_b\n",
    "        # output = activation(input) * ua_w + ua_b\n",
    "        # L2Weight_tmp计算∑(ua_w)^2，应该是L2正则化    \n",
    "        \n",
    "    ua_we = tf.get_variable(\n",
    "            name='ua_we'\n",
    "            #, shape=[hidden_units[hidden_num-1], output_dim_final]\n",
    "            , initializer=w_Univ0[-1]\n",
    "        )\n",
    "    ua_be = tf.get_variable(\n",
    "            name='ua_be'\n",
    "            #, shape=[1,output_dim_final]\n",
    "            , initializer=b_Univ0[-1]\n",
    "        )\n",
    "    \n",
    "    z1 = tf.matmul(output, ua_we)+ua_be\n",
    "    w_Univ.append(ua_we)\n",
    "    b_Univ.append(ua_be)\n",
    "    # 同样，最后一层output = input * ua_we + ua_be",
    "    \n",
    "    # you can ignore this trick for now. Consider ASI=False\n",
    "    if ASI:\n",
    "        output=x0\n",
    "        for i in range(hidden_num):\n",
    "            input_dim = add_hidden[i]\n",
    "            output_dim = add_hidden[i+1]\n",
    "            print('input_dim:%s, output_dim:%s'%(input_dim,output_dim))\n",
    "            name_scope = 'hidden' + np.str(i+1+hidden_num) ,
    "            output,ua_w,ua_b,L2Wight_tmp=add_layer2(output,input_dim,output_dim,isresnet=isresnet,\n",
    "                                               astddev=w_Univ0[i],bstddev=b_Univ0[i], ActFuc=ActFuc,\n",
    "                                               seed=seed, name_scope= name_scope)\n",
    "            \n",
    "            \n",
    "                                               \n",
    "        ua_we = tf.get_variable(\n",
    "                name='ua_wei2'\n",
    "                #, shape=[hidden_units[hidden_num-1], output_dim_final]\n",
    "                , initializer=-w_Univ0[-1]\n",
    "            )\n",
    "        ua_be = tf.get_variable(\n",
    "                name='ua_bei2'\n",
    "                #, shape=[1,output_dim_final]\n",
    "                , initializer=-b_Univ0[-1]\n",
    "            )\n",
    "        z2 = tf.matmul(output, ua_we)+ua_be\n",
    "    else:\n",
    "        z2=0\n",
    "    z=z1+z2\n",
    "    return z,w_Univ,b_Univ,L2w_all\n",
    "    # z是预测结果,w_Univ,b_Univ是计算过程中用到的参数，L2w_all对应w_Univ每一层的L2的值\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.variable_scope('Graph',reuse=tf.AUTO_REUSE) as scope:\n",
    "    # Our inputs will be a batch of values taken by our functions\n",
    "    x = tf.placeholder(tf.float32, shape=[None, R_variable['input_dim']], name=\"x\")\n",
    "    \n",
    "    \n",
    "    y_true = tf.placeholder_with_default(input=[[0.0]], shape=[None, R_variable['output_dim']], name=\"y\")\n",
    "    in_learning_rate= tf.placeholder_with_default(input=1e-3,shape=[],name='lr')\n",
    "    y,_,_,_ = univAprox2(x, R_variable['hidden_units'],input_dim = R_variable['input_dim'],\n",
    "                                            astddev=R_variable['astddev'],bstddev=R_variable['bstddev'],\n",
    "                                            ActFuc=R_variable['ActFuc'],\n",
    "                                            seed=R_variable['seed'],ASI=R_variable['ASI'])\n",
    "                                            # 这里在计算训练值\n",
    "    loss=tf.reduce_mean(tf.square(y_true-y))\n",
    "    loss = tf.reduce_mean(loss)\n",
    "            \n",
    "    # We define our train operation using the Adam optimizer\n",
    "    adam = tf.compat.v1.train.AdamOptimizer(learning_rate=in_learning_rate) \n",
    "    train_op = adam.minimize(loss)\n",
    "    # 这边更新矩阵的参数\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True  \n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()  \n",
    "        \n",
    "class model():\n",
    "    def __init__(self): \n",
    "        R_variable['y_train']=[]\n",
    "        R_variable['y_test']=[]\n",
    "        R_variable['loss_test']=[]\n",
    "        R_variable['loss_train']=[]\n",
    "        # 记录训练结果\n",
    "    \n",
    "        nametmp='%smodel/'%(FolderName)\n",
    "        mkdir(nametmp)\n",
    "        w_Univ0, b_Univ0=getWini(hidden_units=R_variable['hidden_units'],\n",
    "                                 input_dim = R_variable['input_dim'],\n",
    "                                 output_dim_final = R_variable['output_dim'],\n",
    "                                 astddev=R_variable['astddev'],\n",
    "                                 bstddev=R_variable['bstddev'])\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.save(sess, \"%smodel.ckpt\"%(nametmp))\n",
    "        y_test, loss_test_tmp= sess.run([y,loss],feed_dict={x: test_inputs, y_true: R_variable['y_true_test']})\n",
    "        y_train,loss_train_tmp= sess.run([y,loss],feed_dict={x: train_inputs, y_true: R_variable['y_true_train']})\n",
    "        R_variable['y_train']=y_train\n",
    "        R_variable['y_test']=y_test\n",
    "        R_variable['loss_test'].append(loss_test_tmp)\n",
    "        R_variable['loss_train'].append(loss_train_tmp)\n",
    "        \n",
    "        self.ploty(name='ini') \n",
    "        # 初始化，读入数据\n",
    "        \n",
    "    def run_onestep(self):\n",
    "        y_test, loss_test_tmp= sess.run([y,loss],feed_dict={x: test_inputs, y_true: R_variable['y_true_test']})\n",
    "        \n",
    "        y_train,loss_train_tmp= sess.run([y,loss],feed_dict={x: train_inputs, y_true: R_variable['y_true_train']})\n",
    "            \n",
    "        if R_variable['train_size']>R_variable['batch_size']:\n",
    "            indperm = np.random.permutation(R_variable['train_size'])\n",
    "            nrun_epoch=np.int32(R_variable['train_size']/R_variable['batch_size'])\n",
    "            \n",
    "            for ijn in range(nrun_epoch):\n",
    "                ind = indperm[ijn*R_variable['batch_size']:(ijn+1)*R_variable['batch_size']] \n",
    "                _= sess.run(train_op, feed_dict={x: train_inputs[ind], y_true: R_variable['y_true_train'][ind],\n",
    "                                                  in_learning_rate:R_variable['learning_rate']})\n",
    "        else:\n",
    "            _ = sess.run(train_op, feed_dict={x: train_inputs, y_true: R_variable['y_true_train'],\n",
    "                                                  in_learning_rate:R_variable['learning_rate']})\n",
    "        R_variable['learning_rate']=R_variable['learning_rate']*(1-R_variable['learning_rateDecay'])\n",
    "            \n",
    "        R_variable['y_train']=y_train\n",
    "        R_variable['y_test']=y_test\n",
    "        \n",
    "        R_variable['loss_test'].append(loss_test_tmp)\n",
    "        R_variable['loss_train'].append(loss_train_tmp)\n",
    "        # 运行一步，并记录\n",
    "        \n",
    "    def run(self,step_n=1):\n",
    "        nametmp='%smodel/model.ckpt'%(FolderName)\n",
    "        saver.restore(sess, nametmp)\n",
    "        # 每step_n步保存一次\n",
    "        for ii in range(step_n):\n",
    "            self.run_onestep()\n",
    "            if R_variable['loss_train'][-1]<R_variable['tol']:\n",
    "                # 这个错误检测，预防loss太大，浮点数爆炸，符号位被占用导致loss变成负数\n",
    "                print('model end, error:%s'%(R_variable['lossu_train'][-1]))\n",
    "                sheet.write(0,1,R_variable['loss_test'])\n",
    "                sheet.write(1,1,R_variable['loss_train'])\n",
    "                self.plotloss()\n",
    "                \n",
    "                self.ploty()\n",
    "                self.savefile()\n",
    "                  \n",
    "                nametmp='%smodel/'%(FolderName)\n",
    "                shutil.rmtree(nametmp)\n",
    "                saver.save(sess, \"%smodel.ckpt\"%(nametmp))\n",
    "                break\n",
    "            if ii==0:\n",
    "                print('initial %s'%(np.max(R_variable['y_train'])))\n",
    "                \n",
    "            if ii%plotepoch==0:\n",
    "                print('time elapse: %.3f'%(time.time()-t0))\n",
    "                print('model, epoch: %d, test loss: %f' % (ii,R_variable['loss_test'][-1]))\n",
    "                print('model, epoch: %d, train loss: %f' % (ii,R_variable['loss_train'][-1]))\n",
    "                self.plotloss()\n",
    "                L1 = pd.DataFrame(R_variable['loss_test'])\n",
    "                L2 = pd.DataFrame(R_variable['loss_train'])\n",
    "                #将每一次的loss写入excel",
    "                writer = pd.ExcelWriter('hdim5000.xls')\n",
    "                L1.to_excel(writer, 'page_1', float_format='%.5f')\n",
    "                L2.to_excel(writer, 'page_2', float_format='%.5f')\n",
    "                writer.save()\n",
    "                writer.close()\n",
    "                self.ploty(name='%s'%(ii))\n",
    "                self.savefile()\n",
    "                nametmp='%smodel/'%(FolderName)\n",
    "                shutil.rmtree(nametmp)\n",
    "                saver.save(sess, \"%smodel.ckpt\"%(nametmp))\n",
    "                # 画图记录\n",
    "            \n",
    "                \n",
    "    def plotloss(self):\n",
    "        plt.figure()\n",
    "        ax = plt.gca()\n",
    "        y1 = R_variable['loss_test']\n",
    "        y2 = R_variable['loss_train']\n",
    "        plt.plot(y1,'ro',label='Test')\n",
    "        plt.plot(y2,'g*',label='Train')\n",
    "#         ax.set_xscale('log')\n",
    "        ax.set_yscale('log')                \n",
    "        plt.legend(fontsize=18)\n",
    "        plt.title('loss',fontsize=15)\n",
    "        plt.xlabel('epoch',fontsize=15)\n",
    "        fntmp = '%sloss'%(FolderName)\n",
    "        mySaveFig(plt,fntmp,ax=ax,isax=1,iseps=0)\n",
    "        \n",
    "            \n",
    "    def ploty(self,name=''):\n",
    "        \n",
    "        if R_variable['input_dim']==2:\n",
    "            # Make data.\n",
    "            X = np.arange(R_variable['x_start'], R_variable['x_end'], 0.1)\n",
    "            Y = np.arange(R_variable['x_start'], R_variable['x_end'], 0.1)\n",
    "            X, Y = np.meshgrid(X, Y)\n",
    "            xy=np.concatenate((np.reshape(X,[-1,1]),np.reshape(Y,[-1,1])),axis=1)\n",
    "            Z = np.reshape(get_y_func(xy),[len(X),-1])\n",
    "            fp = plt.figure()\n",
    "            ax = fp.gca(projection='3d')\n",
    "            # Plot the surface.\n",
    "            surf = ax.plot_surface(X, Y, Z-np.min(Z), cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
    "            # Customize the z axis.\n",
    "            #ax.set_zlim(-2.01, 2.01)\n",
    "            ax.zaxis.set_major_locator(LinearLocator(5))\n",
    "            ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "            # Add a color bar which maps values to colors.\n",
    "            fp.colorbar(surf, shrink=0.5, aspect=5)\n",
    "            ax.scatter(train_inputs[:,0], train_inputs[:,1], R_variable['y_train']-np.min(R_variable['y_train']))\n",
    "            fntmp = '%s2du%s'%(FolderName,name)\n",
    "            mySaveFig(plt,fntmp,ax=ax,isax=1,iseps=0)\n",
    "        if R_variable['input_dim']==1:\n",
    "            plt.figure()\n",
    "            ax = plt.gca()\n",
    "            y1 = R_variable['y_test']\n",
    "            y2 = R_variable['y_train']\n",
    "            y3 = R_variable['y_true_test']\n",
    "            plt.plot(test_inputs,y1,'ro',label='Test')\n",
    "            plt.plot(train_inputs,y2,'g*',label='Train')\n",
    "            plt.plot(test_inputs,y3,'b*',label='True')\n",
    "            plt.title('g2u',fontsize=15)        \n",
    "            plt.legend(fontsize=18) \n",
    "            fntmp = '%su_m%s'%(FolderName,name)\n",
    "            mySaveFig(plt,fntmp,ax=ax,isax=1,iseps=0)\n",
    "            \n",
    "    def savefile(self):\n",
    "        with open('%s/objs.pkl'%(FolderName), 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "            pickle.dump(R_variable, f, protocol=4)\n",
    "         \n",
    "        text_file = open(\"%s/Output.txt\"%(FolderName), \"w\")\n",
    "        for para in R_variable:\n",
    "            if np.size(R_variable[para])>20:\n",
    "                continue\n",
    "            text_file.write('%s: %s\\n'%(para,R_variable[para]))\n",
    "        \n",
    "        text_file.close()\n",
    "        \n",
    "                \n",
    "            \n",
    "model1=model()\n",
    "model1.run(10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
